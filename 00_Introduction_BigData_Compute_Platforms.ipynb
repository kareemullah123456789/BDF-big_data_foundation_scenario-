{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd9c0e9",
   "metadata": {},
   "source": [
    "# Module 0: Why PySpark? Compute, Platforms & Scaling\n",
    "**Scenario:** Working for a Global Consultant (e.g., Accenture, Deloitte, Capgemini).\n",
    "\n",
    "**Client Situation:** A Retail Client (e.g., Tesco) has 10TB of clickstream logs. They are trying to analyze it using Excel/Python on a laptop, and it keeps crashing.\n",
    "\n",
    "**Key Questions:**\n",
    "1.  **Why PySpark?** Why can't we just use Pandas?\n",
    "2.  **How much Compute do we need?** (Sizing the Cluster).\n",
    "3.  **Which Platform?** (AWS EMR vs Azure Synapse vs Databricks).\n",
    "\n",
    "This notebook explores the *Foundational Theory* before you write code.\n",
    "\n",
    "---\n",
    "## 1. The Scale Problem: \"It works on my machine\"\n",
    "*   **Small Data (Excel):** < 1 Million Rows.\n",
    "*   **Medium Data (Pandas):** < 10 Million Rows (Fits in RAM, e.g., 16GB).\n",
    "*   **Big Data (Spark):** Billions of Rows (TB/PB). Needs **Distributed Computing**.\n",
    "\n",
    "### Scenario: The Crash\n",
    "Your manager asks you to process a 50GB CSV file.\n",
    "*   **Laptop:** 16GB RAM.\n",
    "*   **Process:** Load 50GB into RAM.\n",
    "*   **Result:** `MemoryError` (Crash).\n",
    "\n",
    "### The Solution: Distributed Computing (Spark)\n",
    "Instead of 1 super-computer, use 10 cheap computers (Nodes).\n",
    "*   Split the 50GB file into 10 chunks of 5GB.\n",
    "*   Each computer processes 5GB.\n",
    "*   Combine results.\n",
    "*   **Result:** Success.\n",
    "\n",
    "---\n",
    "## 2. Platforms used in Service Companies\n",
    "You won't install Spark on your laptop in a real job. You will use a **Cloud Platform**.\n",
    "\n",
    "| Platform | Cloud Provider | Common Clients |\n",
    "| :--- | :--- | :--- |\n",
    "| **Databricks** | Azure / AWS / GCP | Everyone (Gold Standard) |\n",
    "| **AWS EMR** (Elastic MapReduce) | AWS | Startups, Tech-focused corps |\n",
    "| **Azure Synapse Analytics** | Azure | Banks, Enterprise Corps (Microsoft shops) |\n",
    "| **Google Dataproc** | GCP | Retailers using Google Analytics |\n",
    "| **Cloudera** | On-Premise | Govt, Old Banks (Security focus) |\n",
    "\n",
    "---\n",
    "## 3. Interactive Compute Sizing Calculator\n",
    "How do you decide how many machines (\"Nodes\") to ask for? \n",
    "*   **Rule of Thumb:** 1 Core can process ~2-4GB of data efficiently in a task.\n",
    "*   **Shuffle Partition:** Aim for 128MB - 200MB per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a350641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive: Compute Scaling Calculator ---\n",
    "# Scenario:\n",
    "# Client Data: 50 GB\n",
    "# Deadline: \"Need this processed in 15 minutes.\"\n",
    "\n",
    "def calculate_cluster_requirements(data_size_gb, processing_time_minutes):\n",
    "    \"\"\"\n",
    "    Very rough estimation based on common cluster performance.\n",
    "    Assumption: 1 Core processes ~10GB/Hour (0.16 GB/Min) in complex ETL.\n",
    "    \"\"\"\n",
    "    processing_speed_per_core_per_min = 0.15 # GB processed per minute per CPU core\n",
    "\n",
    "    total_processing_capacity_needed = data_size_gb / processing_time_minutes\n",
    "    cores_needed = total_processing_capacity_needed / processing_speed_per_core_per_min\n",
    "\n",
    "    print(f\"--- Sizing Report for {data_size_gb} GB Data ---\")\n",
    "    print(f\"Goal: Finish in {processing_time_minutes} minutes.\")\n",
    "    print(f\"Throughput Needed: {total_processing_capacity_needed:.2f} GB/min\")\n",
    "    print(f\"Estimated CPU Cores Needed: {int(cores_needed)}\")\n",
    "\n",
    "    # Typical machines have 4 or 8 cores available for Spark.\n",
    "    nodes_8_core = int(cores_needed / 8) + 1\n",
    "    print(f\"Recommendation: Request a cluster with {nodes_8_core} Workers (8-core machines).\")\n",
    "\n",
    "# Simulate different scenarios\n",
    "calculate_cluster_requirements(50, 60)   # 50GB in 1 Hour\n",
    "print(\"\\n\")\n",
    "calculate_cluster_requirements(1000, 60) # 1TB in 1 Hour\n",
    "print(\"\\n\")\n",
    "calculate_cluster_requirements(5, 120)   # 5GB in 2 Hours (Micro-batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad5653",
   "metadata": {},
   "source": [
    "## 4. When NOT to use Spark (The Anti-Pattern)\n",
    "Many junior engineers use Spark for EVERYTHING.\n",
    "*   **Problem:** 2MB CSV file -> Spin up a 5-node Cluster ($50 cost).\n",
    "*   **Correct Way:** 2MB CSV file -> Use Pandas on a single cheap VM ($0.10 cost).\n",
    "\n",
    "**Interview Tip:** Always say \"I evaluate the Volume, Velocity, and Variety of data before choosing a tool.\"\n",
    "\n",
    "---\n",
    "## 5. Next Steps\n",
    "Now that you understand the **WHY**, proceed to **Module 1** to start coding."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
