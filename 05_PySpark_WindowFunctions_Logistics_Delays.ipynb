{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bfe52a",
   "metadata": {},
   "source": [
    "# Module 5: PySpark Window Functions - Logistics & Time Analysis\n",
    "**Scenario:** Working for a Logistics Company (e.g., FedEx, DHL, Maersk).\n",
    "\n",
    "**Objective:** Analytics on \"Journey Time\". How long does it take for a package to move from Point A to Point B?\n",
    "\n",
    "**The Challenge:**\n",
    "Standard `groupBy` cannot look at \"Relationship between rows\".\n",
    "*   Row 1: Package A scanned at Warehouse (10:00 AM)\n",
    "*   Row 2: Package A scanned at Truck (12:00 PM)\n",
    "*   **Question:** What was the delay? (Row 2 Time - Row 1 Time).\n",
    "\n",
    "**The Solution:** Window Functions (`lag`, `lead`, `rank`).\n",
    "These are the most powerful tools in SQL/Spark for time-series data.\n",
    "\n",
    "---\n",
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec62ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PySpark\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"PySpark is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing PySpark...\")\n",
    "    !pip install pyspark findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Logistics_Window_Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb7d2a",
   "metadata": {},
   "source": [
    "## 2. Load Logistics Tracking Data\n",
    "We track 2 packages (`PKG_123` and `PKG_456`).\n",
    "Each row is a \"Scan Event\".\n",
    "\n",
    "*   `PKG_123`: Warehouse -> Truck -> Distribution Center -> Delivered.\n",
    "*   `PKG_456`: Warehouse -> Truck (Stuck there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistics Tracking Data ---\n",
    "logistics_data = [\n",
    "    # Package 123 Journey\n",
    "    (\"PKG_123\", \"Warehouse_A\", \"2023-01-01 08:00:00\"),\n",
    "    (\"PKG_123\", \"Truck_Dispatch\", \"2023-01-01 10:00:00\"), # 2 hours later\n",
    "    (\"PKG_123\", \"Distribution_Center\", \"2023-01-01 15:00:00\"), # 5 hours later\n",
    "    (\"PKG_123\", \"Delivered\", \"2023-01-01 18:00:00\"),      # 3 hours later\n",
    "\n",
    "    # Package 456 Journey (Problematic)\n",
    "    (\"PKG_456\", \"Warehouse_B\", \"2023-01-02 09:00:00\"),\n",
    "    (\"PKG_456\", \"Truck_Dispatch\", \"2023-01-02 20:00:00\")  # 11 hours delay!\n",
    "]\n",
    "\n",
    "schema = [\"package_id\", \"location\", \"scan_timestamp\"]\n",
    "df_tracking = spark.createDataFrame(logistics_data, schema)\n",
    "\n",
    "# Convert string to TimestampType (Crucial for time math)\n",
    "df_tracking = df_tracking.withColumn(\"scan_timestamp\", to_timestamp(\"scan_timestamp\"))\n",
    "\n",
    "print(\"--- Raw Tracking Data ---\")\n",
    "df_tracking.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492d87e",
   "metadata": {},
   "source": [
    "## 3. Window Function: Calculate \"Time Since Last Checkpoint\"\n",
    "We need to peek at the **Previous Row**'s timestamp.\n",
    "1.  **Partition:** Group by `package_id` (Packages are independent).\n",
    "2.  **Order By:** `scan_timestamp` (Chronological order).\n",
    "3.  **Function:** `lag(\"scan_timestamp\")` retrieves the value from 1 row before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Window\n",
    "# Partition by ID (So PKG_123 doesn't mix with PKG_456)\n",
    "# Order by Time (So we compare 10am to 8am, not random)\n",
    "windowSpec = Window.partitionBy(\"package_id\").orderBy(\"scan_timestamp\")\n",
    "\n",
    "# 2. Get Previous Timestamp using Lag()\n",
    "df_with_lag = df_tracking.withColumn(\"previous_scan_time\", lag(\"scan_timestamp\", 1).over(windowSpec))\n",
    "\n",
    "# 3. Calculate Delay (Current Time - Previous Time)\n",
    "# unix_timestamp converts time to seconds. Difference / 3600 = Hours.\n",
    "df_delays = df_with_lag.withColumn(\n",
    "    \"delay_hours\",\n",
    "    round(\n",
    "        (unix_timestamp(\"scan_timestamp\") - unix_timestamp(\"previous_scan_time\")) / 3600,\n",
    "        2\n",
    "    )\n",
    ").fillna(0.0, subset=[\"delay_hours\"]) # First scan has no delay\n",
    "\n",
    "print(\"--- Tracking Analysis with Delays ---\")\n",
    "df_delays.show(truncate=False)\n",
    "\n",
    "# Look at PKG_456: 11 Hours delay! Management needs to see this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df803760",
   "metadata": {},
   "source": [
    "## 4. Business Insight: Bottleneck Identification\n",
    "We want to flag any operational step that takes > 6 hours as a **\"Major Delay\"**.\n",
    "\n",
    "*   This is called **KPI Monitoring** (Key Performance Indicator).\n",
    "*   Service companies sell dashboards that turn red when this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed383a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Major Delays\n",
    "df_major_delays = df_delays.filter(col(\"delay_hours\") > 6) \\\n",
    "    .select(\"package_id\", \"location\", \"delay_hours\", \"previous_scan_time\")\n",
    "\n",
    "print(\"--- ALERT: Major Fulfillment Delays (> 6 Hours) ---\")\n",
    "df_major_delays.show(truncate=False)\n",
    "\n",
    "# PKG_456 at Truck_Dispatch took 11 hours. Someone forgot to load the truck!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
