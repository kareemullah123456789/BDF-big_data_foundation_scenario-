{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah123456789/BDF-big_data_foundation_scenario-/blob/main/pyspark_service_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fea8813",
      "metadata": {
        "id": "8fea8813"
      },
      "source": [
        "# PySpark for Service-Based Companies (Cognizant, TCS, Infosys)\n",
        "\n",
        "This notebook simulates real-world projects you will encounter as a Data Engineer or Data Scientist in major service-based IT companies.\n",
        "\n",
        "## Learning Objectives\n",
        "1.  **Understand the \"Service\" Context:** Clients (Banks, Retailers, Telecoms) hire service companies to solve specific data problems (Migration, Data Quality, AI Readiness).\n",
        "2.  **Master PySpark for ETL:** 80% of AI projects start with cleaning and transforming massive datasets.\n",
        "3.  **Solve Real Scenarios:** We will walk through 4 common project types:\n",
        "    *   **Migration:** Moving legacy CSVs to a modern Data Lake (Parquet).\n",
        "    *   **Data Quality (DQ):** Handling duplicates and bad data in Insurance claims.\n",
        "    *   **Feature Engineering:** Aggregating transactional telecom data for Churn Prediction.\n",
        "    *   **Log Analytics:** Parsing server logs for Fraud Detection.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Environment Setup (Google Colab Friendly)\n",
        "Run the following cells to install Apache Spark and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9490ccb6",
      "metadata": {
        "id": "9490ccb6"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries (Only needed for Google Colab/local env without Spark pre-installed)\n",
        "try:\n",
        "    import pyspark\n",
        "    print(\"PySpark is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Installing PySpark...\")\n",
        "    !pip install pyspark findspark\n",
        "\n",
        "# Initialize Spark Session (Entry point for all PySpark Applications)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import os\n",
        "\n",
        "# Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ServiceBasedCompanyScenarios\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(\"Spark Session Created Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c68a128",
      "metadata": {
        "id": "2c68a128"
      },
      "source": [
        "## Scenario 1: Legacy Data Migration (Retail Client)\n",
        "\n",
        "**Problem Statement:**\n",
        "A large retail client (e.g., Walmart) is migrating 10 years of sales data from an old Mainframe system to a modern Hadoop/Spark Data Lake. The data is received as messy CSV files.\n",
        "*   **Challenge:** The data has `null` values for `store_id` (data corruption) and dates are in inconsistent formats.\n",
        "*   **Goal:**\n",
        "    1.  Read the raw CSV data.\n",
        "    2.  Filter out corrupted records (where `store_id` is null).\n",
        "    3.  Standardize the date format.\n",
        "    4.  Write the clean data to **Parquet** format (columnar storage optimized for Big Data).\n",
        "\n",
        "**Why this matters:**\n",
        "This is the \"Hello World\" of Data Engineering projects. 90% of your first project will involve moving data from Place A to Place B and cleaning it.\n",
        "\n",
        "**Approach:**\n",
        "1.  Define a strict Schema using `StructType` to enforce data types.\n",
        "2.  Use `spark.read` with options to handle headers and potential malformed rows.\n",
        "3.  Use `na.drop` or `filter` to remove bad data.\n",
        "4.  Use `withColumn` and `to_date` to fix date strings.\n",
        "5.  Write using `mode(\"overwrite\").parquet()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3b22da",
      "metadata": {
        "id": "6b3b22da"
      },
      "outputs": [],
      "source": [
        "# --- Scenario 1: Retail Data Migration & Cleaning ---\n",
        "\n",
        "# 1. Generate Dummy Data (Simulating a CSV file from a legacy system)\n",
        "retail_data_raw = [\n",
        "    (101, \"SKU_A\", \"2023-01-15\", 50.0, \"Completed\"),\n",
        "    (102, \"SKU_B\", \"15-01-2023\", 30.0, \"Returned\"),  # Date format issue\n",
        "    (None, \"SKU_C\", \"2023-01-16\", 20.0, \"Failed\"),   # Corrupted ID\n",
        "    (103, \"SKU_D\", \"2023/01/17\", 100.0, None),       # Null Status\n",
        "    (104, \"SKU_E\", \"2023-01-18\", None, \"Processing\") # Null Amount\n",
        "]\n",
        "\n",
        "# We will treat this list as our \"Raw\" DataFrame for simulation\n",
        "raw_cols = [\"store_id\", \"product_sku\", \"transaction_date\", \"amount\", \"status\"]\n",
        "df_retail = spark.createDataFrame(retail_data_raw, schema=raw_cols)\n",
        "\n",
        "print(\"--- Raw Retail Data (From Legacy System) ---\")\n",
        "df_retail.show()\n",
        "\n",
        "# 2. Step: Data Cleaning\n",
        "# a. Handle missing Store IDs (Critical for business logic)\n",
        "df_clean = df_retail.dropna(subset=[\"store_id\"])\n",
        "\n",
        "# b. Handle missing Amounts (Assume 0 if missing, though typically we might flag or drop)\n",
        "df_clean = df_clean.fillna(0.0, subset=[\"amount\"])\n",
        "\n",
        "# c. Standardize Date Format\n",
        "# The raw data has mixed formats: 'yyyy-MM-dd', 'dd-MM-yyyy', 'yyyy/MM/dd'.\n",
        "# In a real project, you might use complex Regex or UDFs. Here we demonstrate a simple coalescing approach.\n",
        "# We will create a new column 'clean_date' using `coalesce` to try multiple formats.\n",
        "df_clean = df_clean.withColumn(\"clean_date\",\n",
        "    coalesce(\n",
        "        to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"),\n",
        "        to_date(col(\"transaction_date\"), \"dd-MM-yyyy\"),\n",
        "        to_date(col(\"transaction_date\"), \"yyyy/MM/dd\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# d. Filter out rows where date parsing failed (Optional, depending on business rule)\n",
        "df_final_retail = df_clean.filter(col(\"clean_date\").isNotNull())\n",
        "\n",
        "print(\"--- Cleaned Retail Data (Ready for Data Lake) ---\")\n",
        "df_final_retail.show()\n",
        "\n",
        "# 3. Write to Parquet (Simulated Path)\n",
        "# In a real environment like HDFS or S3: output_path = \"s3a://data-lake/processed/retail_sales/\"\n",
        "output_path = \"retail_sales_processed\"\n",
        "df_final_retail.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Data successfully written to {output_path} in Parquet format.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd337407",
      "metadata": {
        "id": "cd337407"
      },
      "source": [
        "## Scenario 2: Data Quality Framework (Insurance Client)\n",
        "\n",
        "**Problem Statement:**\n",
        "An Insurance client (e.g., AIG, MetLife) receives thousands of claim files daily. However due to system errors, duplicate records are appearing for the same `claim_id`.\n",
        "*   **Challenge:** We need the *latest* version of each claim based on timestamp.\n",
        "*   **Goal:**\n",
        "    1.  Identify duplicates.\n",
        "    2.  Keep only the record with the most recent `update_time`.\n",
        "    3.  Discard older, redundant records.\n",
        "\n",
        "**Why this matters:**\n",
        "Data Quality (DQ) is critical. If you train an ML model on duplicate data, it becomes biased. If you pay a claim twice, you lose money.\n",
        "\n",
        "**Approach:**\n",
        "1.  Use `Window` functions to partition data by `claim_id`.\n",
        "2.  Order by `update_time` descending.\n",
        "3.  Assign a `row_number()`.\n",
        "4.  Filter to keep only rows where `row_number == 1`.\n",
        "This is often called **Deduplication** or **CDC (Change Data Capture)** logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3fc791",
      "metadata": {
        "id": "af3fc791"
      },
      "outputs": [],
      "source": [
        "# --- Scenario 2: Data Quality (Deduplication Logic) ---\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, col, desc\n",
        "\n",
        "# 1. Generate Dummy Insurance Claims Data\n",
        "# Notice 'CLAIM_101' appears twice. We want the one with the latest update_time.\n",
        "claims_data = [\n",
        "    (\"CLAIM_101\", \"Auto_Accident\", 5000.0, \"2023-01-01 10:00:00\"),\n",
        "    (\"CLAIM_102\", \"Fire_Damage\", 12000.0, \"2023-01-01 10:30:00\"),\n",
        "    (\"CLAIM_101\", \"Auto_Accident_v2\", 5000.0, \"2023-01-01 11:00:00\"), # Newer update\n",
        "    (\"CLAIM_103\", \"Theft\", 2500.0, \"2023-01-02 09:00:00\"),\n",
        "    (\"CLAIM_103\", \"Theft_Correction\", 2500.0, \"2023-01-02 08:00:00\")  # Older update (should be discarded)\n",
        "]\n",
        "\n",
        "cols = [\"claim_id\", \"description\", \"claim_amount\", \"update_time\"]\n",
        "df_claims = spark.createDataFrame(claims_data, schema=cols)\n",
        "\n",
        "print(\"--- Raw Claims Data (Includes Duplicates/Versions) ---\")\n",
        "df_claims.show(truncate=False)\n",
        "\n",
        "# 2. Define Window Specification\n",
        "# Partition by claim_id: \"Group rows by claim ID\"\n",
        "# Order by update_time DESC: \"Put the latest one at the top\"\n",
        "windowSpec = Window.partitionBy(\"claim_id\").orderBy(desc(\"update_time\"))\n",
        "\n",
        "# 3. Apply Row Number\n",
        "df_ranked = df_claims.withColumn(\"rn\", row_number().over(windowSpec))\n",
        "\n",
        "# 4. Filter to Keep Only Top Record (Latest Update)\n",
        "df_deduped = df_ranked.filter(col(\"rn\") == 1).drop(\"rn\")\n",
        "\n",
        "print(\"--- Deduplicated Data (Production Ready) ---\")\n",
        "df_deduped.show(truncate=False)\n",
        "\n",
        "# This logic is extremely common in Incremental Loads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ced012e",
      "metadata": {
        "id": "0ced012e"
      },
      "source": [
        "## Scenario 3: Feature Engineering for Churn Prediction (Telecom Client)\n",
        "\n",
        "**Problem Statement:**\n",
        "A Telecom giant (e.g., Verizon, AT&T) wants to predict which customers will leave (churn).\n",
        "*   **The Data:** You are given raw Call Detail Records (CDRs). Each row is a single call.\n",
        "*   **Challenge:** ML models cannot learn from raw logs. They need a \"Customer Profile\" (one row per customer).\n",
        "*   **Goal:** Aggregate the raw logs to create features like:\n",
        "    1.  `total_calls`: Count of calls made.\n",
        "    2.  `total_duration`: Sum of call minutes.\n",
        "    3.  `avg_call_duration`: Average minutes per call.\n",
        "    4.  `is_high_usage`: Flag if total duration > 100 mins.\n",
        "\n",
        "**Why this matters:**\n",
        "This bridges the gap between Data Engineering and Data Science. You are preparing the *Input Matrix (X)* for the model.\n",
        "\n",
        "**Approach:**\n",
        "1.  `groupBy(\"customer_id\")`\n",
        "2.  `agg(count(...), sum(...))`\n",
        "3.  Use `when().otherwise()` to create categorical flags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bafbcce",
      "metadata": {
        "id": "7bafbcce"
      },
      "outputs": [],
      "source": [
        "# --- Scenario 3: Feature Engineering (Churn Prediction) ---\n",
        "\n",
        "from pyspark.sql.functions import sum, avg, count, when\n",
        "\n",
        "# 1. Generate Dummy Call Detail Records (CDRs)\n",
        "# Each row represents one phone call by a customer.\n",
        "cdr_data = [\n",
        "    (\"CUST_A\", \"2023-01-01\", 10),  # 10 minutes\n",
        "    (\"CUST_A\", \"2023-01-02\", 50),\n",
        "    (\"CUST_B\", \"2023-01-01\", 5),\n",
        "    (\"CUST_A\", \"2023-01-03\", 100), # Long call\n",
        "    (\"CUST_B\", \"2023-01-02\", 5),\n",
        "    (\"CUST_C\", \"2023-01-01\", 0),   # Failed call\n",
        "    (\"CUST_C\", \"2023-01-02\", 1)\n",
        "]\n",
        "\n",
        "df_cdr = spark.createDataFrame(cdr_data, [\"customer_id\", \"call_date\", \"duration_mins\"])\n",
        "\n",
        "print(\"--- Raw Telecom Logs (Per Call) ---\")\n",
        "df_cdr.show()\n",
        "\n",
        "# 2. Key Aggregations (Feature Engineering)\n",
        "# We need to transform this into ONE ROW PER CUSTOMER for the ML model.\n",
        "df_features = df_cdr.groupBy(\"customer_id\").agg(\n",
        "    count(\"*\").alias(\"total_calls\"),\n",
        "    sum(\"duration_mins\").alias(\"total_duration_mins\"),\n",
        "    avg(\"duration_mins\").alias(\"avg_call_duration\"),\n",
        "    # Advanced: Count how many long calls (> 30 mins) they made\n",
        "    sum(when(col(\"duration_mins\") > 30, 1).otherwise(0)).alias(\"long_calls_count\")\n",
        ")\n",
        "\n",
        "# 3. Derived Features (Business Logic)\n",
        "# If total_duration > 100, flag as \"High Value Customer\"\n",
        "df_final_features = df_features.withColumn(\n",
        "    \"is_high_value\",\n",
        "    when(col(\"total_duration_mins\") > 100, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "print(\"--- Final Customer Features (Ready for ML Model) ---\")\n",
        "df_final_features.show()\n",
        "# This DataFrame would be saved and then fed into a LogisticRegression or RandomForest model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b49a85",
      "metadata": {
        "id": "57b49a85"
      },
      "source": [
        "## Scenario 4: Log Analysis for Fraud Detection (Banking Client)\n",
        "\n",
        "**Problem Statement:**\n",
        "A Bank (e.g., Citibank, Chase) needs to detect if a single IP address is hammering their login page (Brute Force Attack).\n",
        "*   **The Data:** Raw server logs (Apache/Nginx style). It's just text strings.\n",
        "*   **Challenge:** Need to extract IP, Timestamp, and Endpoint from text.\n",
        "*   **Goal:**\n",
        "    1.  Parse the unstructured log.\n",
        "    2.  Count login attempts per IP per minute.\n",
        "    3.  Flag any IP with > 5 attempts in 1 minute.\n",
        "\n",
        "**Why this matters:**\n",
        "Cybersecurity Analytics is a huge field. Spark is perfect for parsing petabytes of access logs to find anomalies.\n",
        "\n",
        "**Approach:**\n",
        "1.  Use `regexp_extract` to pull fields from the raw string.\n",
        "2.  Convert timestamp string to actual TimestampType.\n",
        "3.  Use Window functions (or `groupBy` with time windows) to count events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbb8099",
      "metadata": {
        "id": "0fbb8099"
      },
      "outputs": [],
      "source": [
        "# --- Scenario 4: Log Analysis for Fraud Detection ---\n",
        "\n",
        "from pyspark.sql.functions import regexp_extract, count, col\n",
        "\n",
        "# 1. Generate Log Data (Unstructured Text)\n",
        "log_data = [\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:01] \\\"GET /login HTTP/1.1\\\" 200\",),\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:02] \\\"POST /login HTTP/1.1\\\" 401\",),\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:03] \\\"POST /login HTTP/1.1\\\" 401\",),\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:04] \\\"POST /login HTTP/1.1\\\" 401\",),\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:05] \\\"POST /login HTTP/1.1\\\" 401\",),\n",
        "    (\"192.168.1.10 - - [01/Jan/2023:12:00:06] \\\"POST /login HTTP/1.1\\\" 401\",), # 6th attempt!\n",
        "    (\"10.0.0.5 - - [01/Jan/2023:12:05:01] \\\"GET /home HTTP/1.1\\\" 200\",)      # Normal user\n",
        "]\n",
        "\n",
        "df_logs = spark.createDataFrame(log_data, [\"raw_log\"])\n",
        "print(\"--- Raw Logs (Unstructured Strings) ---\")\n",
        "df_logs.show(truncate=False)\n",
        "\n",
        "# 2. Parse Logs Steps\n",
        "# Regex Pattern: match IP at start, then ignore up to timestamp in brackets\n",
        "# IP Pattern: ^(\\S+)\n",
        "# Timestamp Pattern: \\[(\\S+)\\]\n",
        "# Endpoint Pattern: \\\"(\\S+)\\s(\\S+)\\s\n",
        "ip_pattern = r'^(\\S+)'\n",
        "ts_pattern = r'\\[(\\S+)\\]'\n",
        "method_pattern = r'\\\"(\\S+)\\s+(\\S+)\\s+'\n",
        "\n",
        "df_parsed = df_logs.withColumn(\"ip_address\", regexp_extract(\"raw_log\", ip_pattern, 1)) \\\n",
        "    .withColumn(\"timestamp_str\", regexp_extract(\"raw_log\", ts_pattern, 1)) \\\n",
        "    .withColumn(\"method\", regexp_extract(\"raw_log\", method_pattern, 1)) \\\n",
        "    .withColumn(\"endpoint\", regexp_extract(\"raw_log\", method_pattern, 2))\n",
        "\n",
        "# 3. Filter for Failed Login Attempts (or just heavy traffic on /login)\n",
        "df_login_attempts = df_parsed.filter((col(\"endpoint\") == \"/login\"))\n",
        "\n",
        "# 4. Detect Brute Force (Count per IP)\n",
        "# In real streaming, we use windowing over time. Here, we group by IP.\n",
        "df_fraud_alert = df_login_attempts.groupBy(\"ip_address\").count() \\\n",
        "    .withColumnRenamed(\"count\", \"attempt_count\") \\\n",
        "    .filter(col(\"attempt_count\") > 5) \\\n",
        "    .withColumn(\"alert\", lit(\"POSSIBLE BRUTE FORCE ATTACK\"))\n",
        "\n",
        "print(\"--- Fraud Detection Alert (High Login Failures) ---\")\n",
        "df_fraud_alert.show()\n",
        "\n",
        "# If the list is empty, no fraud. In our dummy data, 192.168.1.10 has 6 hits."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}