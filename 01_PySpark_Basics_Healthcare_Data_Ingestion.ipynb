{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah123456789/BDF-big_data_foundation_scenario-/blob/main/01_PySpark_Basics_Healthcare_Data_Ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea34d88",
      "metadata": {
        "id": "4ea34d88"
      },
      "source": [
        "# Module 1: PySpark Basics - Healthcare Data Ingestion\n",
        "**Scenario:** Working for a Service Company (e.g., Cognizant/Accenture) for a Healthcare Client (e.g., UnitedHealth/CVS).\n",
        "\n",
        "**Objective:** Understand how to ingest raw patient data, validate its structure (Schema), and perform basic cleanup.\n",
        "\n",
        "**Why this matters:**\n",
        "In a real project, you don't start with clean data. You receive massive CSV/JSON files (often Terabytes in size) from legacy systems (Mainframes). Your first job is to \"Ingest\" this data into a Data Lake (HDFS/S3).\n",
        "\n",
        "**Data Volume Context:**\n",
        "*   **Small Data (Excel/Pandas):** < 1-2 GB. Fits in your laptop's RAM.\n",
        "*   **Big Data (Spark):** > 100 GB - Petabytes. Distributed across many servers.\n",
        "*   *In this notebook, we simulate Big Data using small examples for learning.*\n",
        "\n",
        "---\n",
        "## 1. Setup Environment (Google Colab / Local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6eb7fa53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eb7fa53",
        "outputId": "3b576ba4-1276-456d-9c8b-fbca9b9dfb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark is already installed\n"
          ]
        }
      ],
      "source": [
        "# In Real Project: PySpark is pre-installed on the cluster (EMR/Databricks/Cloudera).\n",
        "# In Google Colab or Local Machine: We must install it first.\n",
        "try:\n",
        "    import pyspark\n",
        "    print(\"PySpark is already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing PySpark...\")\n",
        "    !pip install pyspark findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7117e6",
      "metadata": {
        "id": "1c7117e6"
      },
      "source": [
        "## 2. Initialize Spark Session (Entry Point)\n",
        "Every PySpark application starts with a `SparkSession`. It connects your program to the Cluster Manager (YARN/Kubernetes).\n",
        "\n",
        "*   **Master:** `local[*]` means run locally on your machine using *all* available CPU cores. In production, this would be `yarn` or `k8s://...`.\n",
        "*   **AppName:** Name visible in the Spark UI / YARN logs. Important for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8782e579",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8782e579",
        "outputId": "6da4f818-bf2f-43f1-8822-7ee0c6f1466d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 4.0.2\n",
            "Spark Session Created Successfully!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Healthcare_Data_Ingestion_Dev\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(\"Spark Session Created Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35506b2",
      "metadata": {
        "id": "d35506b2"
      },
      "source": [
        "## 3. Data Ingestion Scenario: Raw Patient CSV\n",
        "**The Context:**\n",
        "*   A hospital's patient registration system exports daily data as CSV.\n",
        "*   The data is stored in the \"Raw Zone\" or \"Landing Zone\" of our Data Lake.\n",
        "*   **Problem:** The CSV has no type information (everything is a string). Dates might be messy.\n",
        "\n",
        "### Approach 1: InferSchema (The \"Lazy\" Way)\n",
        "Spark can guess the data types by reading the file twice (once to check types, once to load data).\n",
        "*   **Pros:** Quick for development.\n",
        "*   **Cons:** Very slow for Big Data (reading 1TB twice takes forever). **Never use in Production.**\n",
        "\n",
        "### Approach 2: Define Schema (The \"Professional\" Way)\n",
        "We tell Spark exactly what the columns are.\n",
        "*   **Pros:** Fast (reads once). Fails immediately if data is wrong (Data Quality).\n",
        "*   **Cons:** Need to type out the schema.\n",
        "\n",
        "---\n",
        "Let's first create some dummy CSV data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53723db9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53723db9",
        "outputId": "914e5694-671b-4546-a497-ff9d3af285d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy file at: raw_data/patients_daily_20230101.csv\n",
            "\n",
            "--- Reading with inferSchema=True (Scans file twice) ---\n",
            "root\n",
            " |-- patient_id: string (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- dob: date (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- contact_number: string (nullable = true)\n",
            " |-- last_visit_date: date (nullable = true)\n",
            " |-- diagnosis: string (nullable = true)\n",
            " |-- bill_amount: string (nullable = true)\n",
            "\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "|patient_id|     full_name|       dob|gender|contact_number|last_visit_date|     diagnosis|bill_amount|\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "|      P001|      John Doe|1980-05-15|     M|      555-0101|     2023-01-01|  Hypertension|     150.50|\n",
            "|      P002|    Jane Smith|1992-08-22|     F|      555-0102|     2023-01-02|  Fracture-Arm|    2500.00|\n",
            "|      P003|  Robert Brown|1975-12-10|     M|          NULL|     2022-12-30|Diabetes_Type2|       NULL|\n",
            "|      P004|   Emily Davis|2001-03-30|     F|      555-0104|     2023-01-03|           Flu|      85.00|\n",
            "|      P005|Michael Wilson|1988-11-11|     M|      555-0105|     2023-01-01|  Hypertension|     150.50|\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "\n",
            "\n",
            "--- Reading with Defined Schema (Scans file once) ---\n",
            "root\n",
            " |-- patient_id: string (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- dob: date (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- contact_number: string (nullable = true)\n",
            " |-- last_visit_date: date (nullable = true)\n",
            " |-- diagnosis: string (nullable = true)\n",
            " |-- bill_amount: double (nullable = true)\n",
            "\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "|patient_id|     full_name|       dob|gender|contact_number|last_visit_date|     diagnosis|bill_amount|\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "|      P001|      John Doe|1980-05-15|     M|      555-0101|     2023-01-01|  Hypertension|      150.5|\n",
            "|      P002|    Jane Smith|1992-08-22|     F|      555-0102|     2023-01-02|  Fracture-Arm|     2500.0|\n",
            "|      P003|  Robert Brown|1975-12-10|     M|          NULL|     2022-12-30|Diabetes_Type2|       NULL|\n",
            "|      P004|   Emily Davis|2001-03-30|     F|      555-0104|     2023-01-03|           Flu|       85.0|\n",
            "|      P005|Michael Wilson|1988-11-11|     M|      555-0105|     2023-01-01|  Hypertension|      150.5|\n",
            "+----------+--------------+----------+------+--------------+---------------+--------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Create Dummy CSV Data ---\n",
        "import os\n",
        "\n",
        "# Create folder for raw data\n",
        "os.makedirs(\"raw_data\", exist_ok=True)\n",
        "\n",
        "# Write dummy data to a CSV file\n",
        "raw_csv_path = \"raw_data/patients_daily_20230101.csv\"\n",
        "csv_content = \"\"\"patient_id,full_name,dob,gender,contact_number,last_visit_date,diagnosis,bill_amount\n",
        "P001,John Doe,1980-05-15,M,555-0101,2023-01-01,Hypertension,150.50\n",
        "P002,Jane Smith,1992-08-22,F,555-0102,2023-01-02,Fracture-Arm,2500.00\n",
        "P003,Robert Brown,1975-12-10,M,,2022-12-30,Diabetes_Type2,NULL\n",
        "P004,Emily Davis,2001-03-30,F,555-0104,2023-01-03,Flu,85.00\n",
        "P005,Michael Wilson,1988-11-11,M,555-0105,2023-01-01,Hypertension,150.50\n",
        "\"\"\"\n",
        "\n",
        "with open(raw_csv_path, \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(f\"Created dummy file at: {raw_csv_path}\")\n",
        "\n",
        "# --- Approach 1: Infer Schema (The Easy Way) ---\n",
        "print(\"\\n--- Reading with inferSchema=True (Scans file twice) ---\")\n",
        "df_inferred = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(raw_csv_path)\n",
        "\n",
        "df_inferred.printSchema()\n",
        "df_inferred.show()\n",
        "\n",
        "# --- Approach 2: Explicit Schema (The Production Way) ---\n",
        "# Using StructTypes is slightly verbose but crucial for large pipelines.\n",
        "print(\"\\n--- Reading with Defined Schema (Scans file once) ---\")\n",
        "\n",
        "patient_schema = StructType([\n",
        "    StructField(\"patient_id\", StringType(), False), # Not Nullable\n",
        "    StructField(\"full_name\", StringType(), True),\n",
        "    StructField(\"dob\", DateType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"contact_number\", StringType(), True),\n",
        "    StructField(\"last_visit_date\", DateType(), True),\n",
        "    StructField(\"diagnosis\", StringType(), True),\n",
        "    StructField(\"bill_amount\", DoubleType(), True) # Handle decimals\n",
        "])\n",
        "\n",
        "df_patients = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(patient_schema) \\\n",
        "    .csv(raw_csv_path)\n",
        "\n",
        "df_patients.printSchema()\n",
        "# Notice how 'dob' and 'last_visit_date' are now proper DateTypes, not Strings.\n",
        "df_patients.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMxrMp47zE6_",
        "outputId": "26b7e937-cede-469f-eeba-2d60c27e798b"
      },
      "id": "FMxrMp47zE6_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a040c1f9",
      "metadata": {
        "id": "a040c1f9"
      },
      "source": [
        "## 4. Renaming & Selecting Columns (Standardization)\n",
        "In large organizations, Data Architects define \"Naming Standards\".\n",
        "*   Example: Raw data has `dob`, but the target data warehouse (Hive/Snowflake) requires `date_of_birth`.\n",
        "*   Example: We only need `full_name` and `diagnosis` for a specific report (PII Compliance - don't load unnecessary data).\n",
        "\n",
        "**Task:**\n",
        "1.  Rename `dob` to `date_of_birth`.\n",
        "2.  Rename `bill_amount` to `total_billed_amount`.\n",
        "3.  Filter out patients with `NULL` contact numbers (Data Quality)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c6cdb74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6cdb74e",
        "outputId": "d0d44abe-26a8-4087-b63b-af7a4ea335f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data with Renamed Columns ---\n",
            "+----------+--------------+-------------+------+------------+---------------+--------------+-------------------+\n",
            "|patient_id|     full_name|date_of_birth|gender|phone_number|last_visit_date|     diagnosis|total_billed_amount|\n",
            "+----------+--------------+-------------+------+------------+---------------+--------------+-------------------+\n",
            "|      P001|      John Doe|   1980-05-15|     M|    555-0101|     2023-01-01|  Hypertension|              150.5|\n",
            "|      P002|    Jane Smith|   1992-08-22|     F|    555-0102|     2023-01-02|  Fracture-Arm|             2500.0|\n",
            "|      P003|  Robert Brown|   1975-12-10|     M|        NULL|     2022-12-30|Diabetes_Type2|               NULL|\n",
            "|      P004|   Emily Davis|   2001-03-30|     F|    555-0104|     2023-01-03|           Flu|               85.0|\n",
            "|      P005|Michael Wilson|   1988-11-11|     M|    555-0105|     2023-01-01|  Hypertension|              150.5|\n",
            "+----------+--------------+-------------+------+------------+---------------+--------------+-------------------+\n",
            "\n",
            "--- Pharmacy Report (Selection) ---\n",
            "+----------+--------------+--------------+\n",
            "|patient_id|     full_name|     diagnosis|\n",
            "+----------+--------------+--------------+\n",
            "|      P001|      John Doe|  Hypertension|\n",
            "|      P002|    Jane Smith|  Fracture-Arm|\n",
            "|      P003|  Robert Brown|Diabetes_Type2|\n",
            "|      P004|   Emily Davis|           Flu|\n",
            "|      P005|Michael Wilson|  Hypertension|\n",
            "+----------+--------------+--------------+\n",
            "\n",
            "--- Valid Contacts Only (Null Phones Removed) ---\n",
            "+----------+--------------+-------------+------+------------+---------------+------------+-------------------+\n",
            "|patient_id|     full_name|date_of_birth|gender|phone_number|last_visit_date|   diagnosis|total_billed_amount|\n",
            "+----------+--------------+-------------+------+------------+---------------+------------+-------------------+\n",
            "|      P001|      John Doe|   1980-05-15|     M|    555-0101|     2023-01-01|Hypertension|              150.5|\n",
            "|      P002|    Jane Smith|   1992-08-22|     F|    555-0102|     2023-01-02|Fracture-Arm|             2500.0|\n",
            "|      P004|   Emily Davis|   2001-03-30|     F|    555-0104|     2023-01-03|         Flu|               85.0|\n",
            "|      P005|Michael Wilson|   1988-11-11|     M|    555-0105|     2023-01-01|Hypertension|              150.5|\n",
            "+----------+--------------+-------------+------+------------+---------------+------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Renaming Columns: Use `withColumnRenamed`\n",
        "df_renamed = df_patients \\\n",
        "    .withColumnRenamed(\"dob\", \"date_of_birth\") \\\n",
        "    .withColumnRenamed(\"contact_number\", \"phone_number\") \\\n",
        "    .withColumnRenamed(\"bill_amount\", \"total_billed_amount\")\n",
        "\n",
        "# 2. Selecting Specific Columns: Use `select`\n",
        "# Business Requirement: Provide a list for the Pharmacy system (only names & diagnosis needed)\n",
        "df_pharmacy = df_renamed.select(\"patient_id\", \"full_name\", \"diagnosis\")\n",
        "\n",
        "# 3. Filtering: Use `filter` or `where`\n",
        "# Data Quality Check: Remove records with NULL or Empty phone numbers\n",
        "df_valid_contacts = df_renamed.filter(col(\"phone_number\").isNotNull())\n",
        "\n",
        "print(\"--- Data with Renamed Columns ---\")\n",
        "df_renamed.show()\n",
        "\n",
        "print(\"--- Pharmacy Report (Selection) ---\")\n",
        "df_pharmacy.show()\n",
        "\n",
        "print(\"--- Valid Contacts Only (Null Phones Removed) ---\")\n",
        "df_valid_contacts.show()\n",
        "# Notice patient P003 (Robert Brown) is removed because contact_number was missing."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}