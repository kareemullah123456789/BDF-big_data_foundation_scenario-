{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea34d88",
   "metadata": {},
   "source": [
    "# Module 1: PySpark Basics - Healthcare Data Ingestion\n",
    "**Scenario:** Working for a Service Company (e.g., Cognizant/Accenture) for a Healthcare Client (e.g., UnitedHealth/CVS).\n",
    "\n",
    "**Objective:** Understand how to ingest raw patient data, validate its structure (Schema), and perform basic cleanup.\n",
    "\n",
    "**Why this matters:**\n",
    "In a real project, you don't start with clean data. You receive massive CSV/JSON files (often Terabytes in size) from legacy systems (Mainframes). Your first job is to \"Ingest\" this data into a Data Lake (HDFS/S3).\n",
    "\n",
    "**Data Volume Context:**\n",
    "*   **Small Data (Excel/Pandas):** < 1-2 GB. Fits in your laptop's RAM.\n",
    "*   **Big Data (Spark):** > 100 GB - Petabytes. Distributed across many servers.\n",
    "*   *In this notebook, we simulate Big Data using small examples for learning.*\n",
    "\n",
    "---\n",
    "## 1. Setup Environment (Google Colab / Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Real Project: PySpark is pre-installed on the cluster (EMR/Databricks/Cloudera).\n",
    "# In Google Colab or Local Machine: We must install it first.\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"PySpark is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing PySpark...\")\n",
    "    !pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7117e6",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session (Entry Point)\n",
    "Every PySpark application starts with a `SparkSession`. It connects your program to the Cluster Manager (YARN/Kubernetes).\n",
    "\n",
    "*   **Master:** `local[*]` means run locally on your machine using *all* available CPU cores. In production, this would be `yarn` or `k8s://...`.\n",
    "*   **AppName:** Name visible in the Spark UI / YARN logs. Important for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8782e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Healthcare_Data_Ingestion_Dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35506b2",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion Scenario: Raw Patient CSV\n",
    "**The Context:**\n",
    "*   A hospital's patient registration system exports daily data as CSV.\n",
    "*   The data is stored in the \"Raw Zone\" or \"Landing Zone\" of our Data Lake.\n",
    "*   **Problem:** The CSV has no type information (everything is a string). Dates might be messy.\n",
    "\n",
    "### Approach 1: InferSchema (The \"Lazy\" Way)\n",
    "Spark can guess the data types by reading the file twice (once to check types, once to load data).\n",
    "*   **Pros:** Quick for development.\n",
    "*   **Cons:** Very slow for Big Data (reading 1TB twice takes forever). **Never use in Production.**\n",
    "\n",
    "### Approach 2: Define Schema (The \"Professional\" Way)\n",
    "We tell Spark exactly what the columns are.\n",
    "*   **Pros:** Fast (reads once). Fails immediately if data is wrong (Data Quality).\n",
    "*   **Cons:** Need to type out the schema.\n",
    "\n",
    "---\n",
    "Let's first create some dummy CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53723db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Dummy CSV Data ---\n",
    "import os\n",
    "\n",
    "# Create folder for raw data\n",
    "os.makedirs(\"raw_data\", exist_ok=True)\n",
    "\n",
    "# Write dummy data to a CSV file\n",
    "raw_csv_path = \"raw_data/patients_daily_20230101.csv\"\n",
    "csv_content = \"\"\"patient_id,full_name,dob,gender,contact_number,last_visit_date,diagnosis,bill_amount\n",
    "P001,John Doe,1980-05-15,M,555-0101,2023-01-01,Hypertension,150.50\n",
    "P002,Jane Smith,1992-08-22,F,555-0102,2023-01-02,Fracture-Arm,2500.00\n",
    "P003,Robert Brown,1975-12-10,M,,2022-12-30,Diabetes_Type2,NULL\n",
    "P004,Emily Davis,2001-03-30,F,555-0104,2023-01-03,Flu,85.00\n",
    "P005,Michael Wilson,1988-11-11,M,555-0105,2023-01-01,Hypertension,150.50\n",
    "\"\"\"\n",
    "\n",
    "with open(raw_csv_path, \"w\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(f\"Created dummy file at: {raw_csv_path}\")\n",
    "\n",
    "# --- Approach 1: Infer Schema (The Easy Way) ---\n",
    "print(\"\\n--- Reading with inferSchema=True (Scans file twice) ---\")\n",
    "df_inferred = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(raw_csv_path)\n",
    "\n",
    "df_inferred.printSchema()\n",
    "df_inferred.show()\n",
    "\n",
    "# --- Approach 2: Explicit Schema (The Production Way) ---\n",
    "# Using StructTypes is slightly verbose but crucial for large pipelines.\n",
    "print(\"\\n--- Reading with Defined Schema (Scans file once) ---\")\n",
    "\n",
    "patient_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), False), # Not Nullable\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"contact_number\", StringType(), True),\n",
    "    StructField(\"last_visit_date\", DateType(), True),\n",
    "    StructField(\"diagnosis\", StringType(), True),\n",
    "    StructField(\"bill_amount\", DoubleType(), True) # Handle decimals\n",
    "])\n",
    "\n",
    "df_patients = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(patient_schema) \\\n",
    "    .csv(raw_csv_path)\n",
    "\n",
    "df_patients.printSchema()\n",
    "# Notice how 'dob' and 'last_visit_date' are now proper DateTypes, not Strings.\n",
    "df_patients.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040c1f9",
   "metadata": {},
   "source": [
    "## 4. Renaming & Selecting Columns (Standardization)\n",
    "In large organizations, Data Architects define \"Naming Standards\".\n",
    "*   Example: Raw data has `dob`, but the target data warehouse (Hive/Snowflake) requires `date_of_birth`.\n",
    "*   Example: We only need `full_name` and `diagnosis` for a specific report (PII Compliance - don't load unnecessary data).\n",
    "\n",
    "**Task:**\n",
    "1.  Rename `dob` to `date_of_birth`.\n",
    "2.  Rename `bill_amount` to `total_billed_amount`.\n",
    "3.  Filter out patients with `NULL` contact numbers (Data Quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Renaming Columns: Use `withColumnRenamed`\n",
    "df_renamed = df_patients \\\n",
    "    .withColumnRenamed(\"dob\", \"date_of_birth\") \\\n",
    "    .withColumnRenamed(\"contact_number\", \"phone_number\") \\\n",
    "    .withColumnRenamed(\"bill_amount\", \"total_billed_amount\")\n",
    "\n",
    "# 2. Selecting Specific Columns: Use `select`\n",
    "# Business Requirement: Provide a list for the Pharmacy system (only names & diagnosis needed)\n",
    "df_pharmacy = df_renamed.select(\"patient_id\", \"full_name\", \"diagnosis\")\n",
    "\n",
    "# 3. Filtering: Use `filter` or `where`\n",
    "# Data Quality Check: Remove records with NULL or Empty phone numbers\n",
    "df_valid_contacts = df_renamed.filter(col(\"phone_number\").isNotNull())\n",
    "\n",
    "print(\"--- Data with Renamed Columns ---\")\n",
    "df_renamed.show()\n",
    "\n",
    "print(\"--- Pharmacy Report (Selection) ---\")\n",
    "df_pharmacy.show()\n",
    "\n",
    "print(\"--- Valid Contacts Only (Null Phones Removed) ---\")\n",
    "df_valid_contacts.show()\n",
    "# Notice patient P003 (Robert Brown) is removed because contact_number was missing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
