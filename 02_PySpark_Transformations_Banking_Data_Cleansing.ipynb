{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b749e573",
   "metadata": {},
   "source": [
    "# Module 2: PySpark Transformations - Banking Data Cleansing\n",
    "**Scenario:** Working for a FinTech / Banking Client (e.g., Chase, Wells Fargo, PayPal).\n",
    "\n",
    "**Objective:** Clean dirty transactional data. Real-world data is never clean; it has negative amounts, null IDs, and weird strings.\n",
    "\n",
    "**The \"Silver Layer\" Concept:**\n",
    "In a Modern Data Lakehouse (Databricks/Delta Lake), we have 3 layers:\n",
    "1.  **Bronze (Raw):** The data exactly as it came from the source (csv/json).\n",
    "2.  **Silver (Cleaned):** Data with types fixed, nulls handled, and duplicates removed. **<- WE ARE HERE**\n",
    "3.  **Gold (Aggregated):** Business-level reports (e.g., Monthly Sales).\n",
    "\n",
    "---\n",
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PySpark\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"PySpark is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing PySpark...\")\n",
    "    !pip install pyspark findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Banking_Data_Cleanser\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc6e06",
   "metadata": {},
   "source": [
    "## 2. Load \"Dirty\" Raw Data\n",
    "We will generate a specialized \"dirty\" dataset containing common banking data issues:\n",
    "*   **Duplicate** transactions.\n",
    "*   **Negative** amounts (e.g., -500.0 instead of 500.0).\n",
    "*   **Null** Customer IDs.\n",
    "*   **Inconsistent** Currency codes ('USD', 'usd', ' Usd ')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15657f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Dirty Transaction Data ---\n",
    "txn_data = [\n",
    "    (\"TXN001\", \"CUST_A\", \"2023-01-01\", 1000.0, \"USD\"),\n",
    "    (\"TXN002\", \"CUST_B\", \"2023-01-02\", -500.0, \"usd\"),     # Issue: Negative Amount, Lowercase Currency\n",
    "    (\"TXN003\", None, \"2023-01-03\", 250.0, \"EUR\"),          # Issue: Missing Customer\n",
    "    (\"TXN004\", \"CUST_C\", \"2023-01-03\", 100.0, \" Usd \"),    # Issue: Spaces in Currency\n",
    "    (\"TXN001\", \"CUST_A\", \"2023-01-01\", 1000.0, \"USD\"),     # Issue: Exact Duplicate of TXN001\n",
    "    (\"TXN005\", \"CUST_D\", \"2023/01/05\", 1200.0, \"USD\"),     # Issue: Bad Date Format\n",
    "    (\"TXN006\", \"CUST_E\", \"2023-01-06\", None, \"USD\")        # Issue: Null Amount\n",
    "]\n",
    "\n",
    "schema = [\"txn_id\", \"customer_id\", \"txn_date\", \"amount\", \"currency\"]\n",
    "df_raw = spark.createDataFrame(txn_data, schema=schema)\n",
    "\n",
    "print(\"--- Raw / Dirty Data ---\")\n",
    "df_raw.show()\n",
    "\n",
    "print(\"\\n--- Summary of Raw Data issues ---\")\n",
    "# describe() is great for seeing mean/max/min (helps catch negatives)\n",
    "df_raw.describe([\"amount\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d487c4",
   "metadata": {},
   "source": [
    "## 3. Cleaning Task 1: Handling Duplicates & Nulls\n",
    "**Rule:**\n",
    "1.  If `customer_id` is missing, we cannot trace the transaction (Audit Risk). **Drop** these rows.\n",
    "2.  If `amount` is missing (Null), replace with **0.0** (Default Value).\n",
    "3.  Remove exact duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop Duplicates\n",
    "df_no_dupes = df_raw.dropDuplicates([\"txn_id\"]) # Deduplicate based on Unique Key\n",
    "\n",
    "# 2. Drop rows where customer_id is NULL\n",
    "# subset parameter tells Spark which column to check for nulls\n",
    "df_valid_cust = df_no_dupes.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# 3. Fill Null Amounts with 0.0\n",
    "# Only fills columns that match the type (double)\n",
    "df_filled = df_valid_cust.fillna(0.0, subset=[\"amount\"])\n",
    "\n",
    "print(\"--- After Removing Duplicates & Handling Nulls ---\")\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00f600",
   "metadata": {},
   "source": [
    "## 4. Cleaning Task 2: Data Standardization (Fixing Values)\n",
    "**The Problem:**\n",
    "*   `currency` is messy (\"usd\", \"USD\", \" Usd \").\n",
    "*   `amount` is negative (-500.0). Transactions in this table should be absolute values.\n",
    "*   `txn_date` has mixed formats.\n",
    "\n",
    "**Tools:**\n",
    "*   `trim()`: Removes leading/trailing spaces.\n",
    "*   `upper()`: Converts to uppercase.\n",
    "*   `when().otherwise()`: Like IF-ELSE in SQL/Excel.\n",
    "*   `abs()`: Absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clean Currency: Trim spaces -> Convert to Upper Case\n",
    "df_std_currency = df_filled.withColumn(\"currency_clean\", upper(trim(col(\"currency\")))) \\\n",
    "                           .drop(\"currency\") # Drop the old messy column\n",
    "\n",
    "# 2. Fix Negative Amounts: Use abs() or when()\n",
    "# If amount < 0, multiply by -1\n",
    "df_std_amount = df_std_currency.withColumn(\"amount_clean\", abs(col(\"amount\"))) \\\n",
    "                               .drop(\"amount\")\n",
    "\n",
    "# 3. Standardize Date\n",
    "# Spark 3.0+ introduced strict Prohibitive Date Parsing.\n",
    "# Even with LEGACY mode, some environments struggle.\n",
    "# The safest modern way is to use `to_date` but handling formats carefully.\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# We will check specifically for the format that is failing '2023/01/05'\n",
    "# In newer Spark versions, yyyy/MM/dd might require specific handling or clean strings first.\n",
    "\n",
    "df_clean_final = df_std_amount.withColumn(\"txn_date_clean\",\n",
    "    coalesce(\n",
    "        to_date(col(\"txn_date\"), \"yyyy-MM-dd\"),\n",
    "        to_date(col(\"txn_date\"), \"yyyy/MM/dd\"),\n",
    "        to_date(col(\"txn_date\"), \"MM/dd/yyyy\") # Adding another potential format just in case\n",
    "    )\n",
    ").drop(\"txn_date\")\n",
    "\n",
    "print(\"--- Final Cleaned Data (Standardized) ---\")\n",
    "df_clean_final.show()\n",
    "\n",
    "# Notice: All currencies are \"USD\", no negative amounts, dates are uniform YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692a85b",
   "metadata": {},
   "source": [
    "## 5. Write to Parquet (The Industry Standard)\n",
    "Why Parquet?\n",
    "1.  **Compression:** 1TB CSV -> ~100GB Parquet. Huge cost savings on Cloud Storage (S3/Azure Blob).\n",
    "2.  **Speed:** It is Columnar. If you select just `amount`, it only reads that one column, skipping the rest. CSV reads everything.\n",
    "3.  **Schema Preservation:** Remembers that `amount` is a Double, not a String.\n",
    "\n",
    "**Action:** Write the clean DataFrame to a folder named `clean_transactions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"clean_transactions\"\n",
    "\n",
    "# Mode 'overwrite' replaces existing data. 'append' adds to it.\n",
    "df_clean_final.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Data successfully written to {output_path}\")\n",
    "\n",
    "# --- Verification (Read it back) ---\n",
    "df_read_back = spark.read.parquet(output_path)\n",
    "print(\"\\n--- Verified Data from Parquet ---\")\n",
    "df_read_back.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
